# Streaming Speech-to-Text

Stream audio in real-time and receive instant transcriptions with Waves STT WebSocket API.

---

## Endpoint

```
wss://waves-api.smallest.ai/api/v1/asr
```

**Method:** WebSocket Connection

---

## Authentication

Include your API key as a query parameter when establishing the WebSocket connection:

```
wss://waves-api.smallest.ai/api/v1/asr?api_key=YOUR_API_KEY
```

<Warning>
Streaming STT requires an Enterprise Monthly or Enterprise Yearly subscription.
</Warning>

---

## Required Parameters

Configure these parameters in the WebSocket URL query string:

| Parameter | Type | Description |
|-----------|------|-------------|
| `api_key` | string | Your Waves API authentication key |
| `audioEncoding` | string | Audio format: `linear16`, `flac`, `mulaw`, or `opus` |
| `audioSampleRate` | string | Sample rate in Hz (recommended: `16000`) |
| `audioChannels` | string | Number of channels (`1` for mono) |
| `audioLanguage` | string | Language code (e.g., `en`, `hi`, `es`) |
| `addPunctuation` | string | Enable punctuation (`true` or `false`) |

---

## Quick Start Example

### Python WebSocket Client

```python
import asyncio
import websockets
import json

async def transcribe_audio():
    # Build connection URL with parameters
    base_url = "wss://waves-api.smallest.ai/api/v1/asr"
    params = {
        "api_key": "YOUR_API_KEY",
        "audioEncoding": "linear16",
        "audioSampleRate": "16000",
        "audioChannels": "1",
        "audioLanguage": "en",
        "addPunctuation": "true"
    }
    
    query = "&".join([f"{k}={v}" for k, v in params.items()])
    url = f"{base_url}?{query}"
    
    # Connect and stream audio
    async with websockets.connect(url) as ws:
        print("‚úÖ Connected to Waves STT")
        
        # Open audio file
        with open("audio.wav", "rb") as f:
            audio_data = f.read()
        
        # Stream in 100ms chunks (3200 bytes for 16kHz)
        chunk_size = 3200
        
        for i in range(0, len(audio_data), chunk_size):
            chunk = audio_data[i:i + chunk_size]
            await ws.send(chunk)
            
            # Receive transcription results
            try:
                response = await asyncio.wait_for(ws.recv(), timeout=0.01)
                result = json.loads(response)
                if "text" in result:
                    print(f"üìù {result['text']}")
            except asyncio.TimeoutError:
                continue
        
        # Signal end of stream
        await ws.send(b'')
        print("‚úÖ Streaming complete")

asyncio.run(transcribe_audio())
```

---

## Sample Response

When speech is detected, you'll receive JSON responses:

```json
{
  "text": "Hello, this is a test transcription.",
  "isEndOfTurn": false
}
```

**Fields:**
- `text` ‚Äî Transcribed text segment
- `isEndOfTurn` ‚Äî `true` when speaker has finished (after silence threshold)

---

## Usage Tips

<Tip>
**For optimal latency:** Stream audio in 100-300ms chunks. For 16kHz audio, use chunks of 3,200-9,600 bytes.

**Formula:** `sample_rate √ó 2 √ó duration_in_seconds`
</Tip>

<Tip>
**Always send an empty message** (`b''`) when finished streaming to flush the remaining audio buffer and receive final transcriptions.
</Tip>

<Info>
Mismatched sample rates cause poor transcription quality. Ensure your audio file's actual sample rate matches the `audioSampleRate` parameter.
</Info>

---

## Next Steps

- Explore [30+ supported languages](https://waves-docs.smallest.ai) including Hindi, Spanish, French, and more
- Learn about [audio format support](https://waves-docs.smallest.ai) (linear16, FLAC, Œº-law, Opus)
- Review [best practices](https://waves-docs.smallest.ai) for production deployments